# æª”æ¡ˆåç¨±: Home.py
# (æ­¤ç‰ˆæœ¬å·²ç§»é™¤ä»‹ç´¹é ï¼Œç›´æ¥é€²å…¥ä¸»æ‡‰ç”¨ç¨‹å¼ä»¥æå‡æ€§èƒ½)

import streamlit as st
import os
import tempfile
from dotenv import load_dotenv

# LangChain å…ƒä»¶
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

# å…¶ä»–å·¥å…·
import pandas as pd
import json
from datetime import datetime
from difflib import get_close_matches
from streamlit_option_menu import option_menu

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="AI åˆç´„å‹•æ…‹æ¯”å°å·¥å…·", layout="wide")

# --- 2. ç’°å¢ƒè®Šæ•¸èˆ‡æ ¸å¿ƒè¨­å®š ---
load_dotenv()
INDEX_NAME = "contract-assistant"

# --- 3. Session State åˆå§‹åŒ– ---
# (ç§»é™¤äº† app_mode ç›¸é—œçš„ state)
if 'comparison_results' not in st.session_state:
    st.session_state.comparison_results = None

if "processed_namespaces" not in st.session_state:
    st.session_state.processed_namespaces = []
if "selected_namespace" not in st.session_state:
    st.session_state.selected_namespace = None
if "core_points_text" not in st.session_state:
    st.session_state.core_points_text = ""

if "search_history" not in st.session_state:
    st.session_state.search_history = [
        {"id":"q-001","query":"æ˜Ÿå®‡èˆªç©ºåˆç´„åˆ†æ","timestamp":"2025-06-28T10:34:00", "results":[{"title":"Q2 æ‘˜è¦","snippet":"ç‡Ÿæ”¶å¹´å¢ 12%","path":"reports/q2_2024.pdf"}], "tags":["finance","q2"],"pinned":True,"notes":"è‘£äº‹æœƒç°¡å ±ç”¨"},
        {"id":"q-002","query":"å®¢æˆ¶æµå¤±å„€è¡¨æ¿","timestamp":"2025-07-03T14:09:20", "results":[{"title":"Cohort åˆ†æ","snippet":"Janâ€“Jun","path":"dashboards/churn.html"}], "tags":["product","retention"],"pinned":False,"notes":""},
    ]

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---
@st.cache_resource
def get_pinecone_client():
    return Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

def fetch_pinecone_namespaces(index_name):
    pc = get_pinecone_client()
    try:
        index_stats = pc.describe_index(index_name).stats
        return list(index_stats.namespaces.keys()) if index_stats and index_stats.namespaces else []
    except Exception:
        return []

if not st.session_state.processed_namespaces:
    st.session_state.processed_namespaces = fetch_pinecone_namespaces(INDEX_NAME)

def process_and_ingest_reference_file(uploaded_file):
    namespace = uploaded_file.name
    with st.spinner(f"æ­£åœ¨è™•ç†åƒè€ƒæ–‡ä»¶ '{namespace}' ä¸¦å­˜å…¥æ°¸ä¹…çŸ¥è­˜åº«..."):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
        docs = text_splitter.split_documents(documents)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        PineconeVectorStore.from_documents(docs, embedding=embeddings, index_name=INDEX_NAME, namespace=namespace)
        os.remove(tmp_file_path)
    st.success(f"åƒè€ƒæ–‡ä»¶ '{namespace}' å·²æˆåŠŸå­˜å…¥çŸ¥è­˜åº«ï¼")
    if namespace not in st.session_state.processed_namespaces:
        st.session_state.processed_namespaces.append(namespace)
    st.cache_data.clear()

@st.cache_resource
def load_and_process_pdf_for_faiss(_uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(_uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    os.remove(tmp_file_path)
    return vectorstore.as_retriever(search_kwargs={'k': 2})

def run_comparison(template_retriever, uploaded_retriever, review_points, temperature, max_tokens):
    llm = ChatOpenAI(model_name='gpt-4o', temperature=temperature, max_tokens=max_tokens)
    comparison_template = """
    ä½ æ˜¯ä¸€ä½é ‚å°–çš„æ³•å‹™å°ˆå®¶ï¼Œä½ çš„ä»»å‹™æ˜¯ç²¾æº–æ¯”è¼ƒå…©ä»½åˆç´„ä¸­çš„åŒä¸€æ¢æ¬¾ï¼Œä¸¦ä»¥ä¿è­·ã€Œæˆ‘æ–¹å…¬å¸ã€çš„åˆ©ç›Šç‚ºæœ€é«˜åŸå‰‡ã€‚
    **æˆ‘æ–¹å…¬å¸çš„æ¨™æº–ç¯„æœ¬æ¢æ¬¾:**
    ```{template_clause}```
    **å¾…å¯©æ–‡ä»¶çš„å°æ‡‰æ¢æ¬¾:**
    ```{uploaded_clause}```
    è«‹é‡å°ã€Œ{topic}ã€é€™å€‹å¯©æŸ¥é‡é»ï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
    1. **æ¢æ¬¾æ‘˜è¦**, 2. **å·®ç•°åˆ†æ**, 3. **é¢¨éšªæç¤ºèˆ‡å»ºè­°**ã€‚
    è«‹ç”¨ Markdown æ ¼å¼æ¸…æ™°åœ°å‘ˆç¾ä½ çš„åˆ†æå ±å‘Šã€‚
    """
    prompt = PromptTemplate.from_template(comparison_template)
    chain = prompt | llm | StrOutputParser()
    results = {}
    progress_bar_main = st.progress(0, text="é–‹å§‹é€²è¡Œæ¯”å°...")
    for i, topic in enumerate(review_points):
        template_clause_docs = template_retriever.invoke(topic)
        uploaded_clause_docs = uploaded_retriever.invoke(topic)
        template_clause_text = "\n---\n".join([doc.page_content for doc in template_clause_docs])
        uploaded_clause_text = "\n---\n".join([doc.page_content for doc in uploaded_clause_docs])
        result = chain.invoke({ "topic": topic, "template_clause": template_clause_text, "uploaded_clause": uploaded_clause_text })
        results[topic] = result
        progress_bar_main.progress((i + 1) / len(review_points), text=f"æ­£åœ¨åˆ†æ: {topic}")
    progress_bar_main.empty()
    return results

# --- 5. UI æ¸²æŸ“å‡½å¼ ---

# (å‡½å¼ draw_intro_page() å·²è¢«å®Œæ•´åˆªé™¤)

def draw_main_app():
    """ç¹ªè£½ä¸»æ‡‰ç”¨ç¨‹å¼ (ç²¾ç¢ºå¾©åˆ» CB_Mainapp2.2.py)"""
    st.title("ğŸš€ AI åˆç´„å‹•æ…‹æ¯”å°å·¥å…·")
    st.markdown("##### æ‚¨å¯ä»¥ä¸Šå‚³åƒè€ƒæ–‡ä»¶ä½œç‚ºæ°¸ä¹…æ¯”å°åŸºæº–ï¼Œç„¶å¾Œä¸Šå‚³å¾…å¯©æ–‡ä»¶é€²è¡Œå³æ™‚åˆ†æã€‚You may upload a reference document to serve as a permanent comparison baseline, and subsequently upload the document under review for real-time analysis.")
    
    def _find_by_id(qid):
        for it in st.session_state.search_history:
            if it["id"] == qid: return it
        return None
    def _df(items):
        return pd.DataFrame([{
            "id": it["id"], 
            "query": it["query"], 
            "timestamp": it["timestamp"], 
            "tags": ", ".join(it.get("tags", [])), "pinned": it.get("pinned", False), 
            "top_title": (it["results"][0]["title"] if it.get("results") else ""),
            "top_path": (it["results"][0]["path"] if it.get("results") else ""),} for it in items])

# --- 6. ä¸»é‚è¼¯ ---
# (ç°¡åŒ–ä¸»é‚è¼¯ï¼Œç›´æ¥åŸ·è¡Œä¸»æ‡‰ç”¨ç¨‹å¼)
draw_main_app()