import os
import tempfile
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# LangChain / Vector DB
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader # <--- [æ–°å¢] ç‚ºäº†è®€å–æš«å­˜æª”
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain.retrievers.multi_query import MultiQueryRetriever

# --- [æ–°å¢] å°å…¥ S3 å„²å­˜èˆ‡ Pinecone å­¸ç¿’çš„å·¥å…·å‡½å¼åº« ---
import storage_utils as storage
from utils import ingest_docs_to_pinecone


# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI åˆç´„åˆå¯©", layout="wide", page_icon="ğŸ“")
st.logo("logo.png")


# --- ç’°å¢ƒè®Šæ•¸èˆ‡æ ¸å¿ƒè¨­å®š ---
load_dotenv()
# --- [æ–°å¢] AI å­¸ç¿’ç”¨çš„æ ¸å¿ƒè¨­å®š ---
LEARNING_NAMESPACE = "approved-analyses"
INDEX_NAME = "contract-assistant"

# --- ã€æ–°å¢ã€‘åˆå§‹åŒ–æ¨¡å‹åƒæ•¸çš„ Session State ---
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.3 # å»ºè­°çš„é è¨­å€¼
if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 2048 # å»ºè­°çš„é è¨­å€¼

# -----------------------------
# Helpers
# -----------------------------

@st.cache_data(show_spinner=False)
def get_language(text_snippet: str, _llm):
    """ä½¿ç”¨ LLM å¿«é€Ÿæª¢æ¸¬æ–‡å­—èªè¨€"""
    if not text_snippet.strip():
        return "unknown"
    try:
        prompt = PromptTemplate.from_template("Detect the primary language of the following text. Respond with only the two-letter ISO 639-1 code (e.g., 'en' for English, 'zh' for Chinese). Text: ```{text}```")
        chain = prompt | _llm | StrOutputParser()
        sample = text_snippet[:200]
        lang_code = chain.invoke({"text": sample})
        return lang_code.lower()
    except Exception:
        return "unknown"

@st.cache_data(show_spinner=False)
def translate_to_chinese(text_to_translate: str, _llm):
    """ä½¿ç”¨ LLM å°‡æ–‡å­—ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡"""
    if not text_to_translate.strip():
        return ""
    try:
        prompt = PromptTemplate.from_template("Please translate the following legal text into Traditional Chinese. Only return the translated text, without any explanation or preamble. Text: ```{text}```")
        chain = prompt | _llm | StrOutputParser()
        return chain.invoke({"text": text_to_translate})
    except Exception as e:
        st.markdown(f"<span style='color:white'>ç¿»è­¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}</span>", unsafe_allow_html=True)
        return text_to_translate

def run_comparison(template_retriever, uploaded_retriever, review_points, temperature, max_tokens):
    """
    åŸ·è¡Œåˆç´„æ¯”å°ï¼Œä¸¦å›å‚³åŒ…å«ã€Œæ‘˜è¦çŸ©é™£ã€èˆ‡ã€Œè©³ç´°å ±å‘Šã€çš„å­—å…¸ã€‚
    """
    llm = ChatOpenAI(model_name='gpt-4o', temperature=temperature, max_tokens=max_tokens)
    
    # --- é«˜å“è³ªæ‘˜è¦çŸ©é™£ Prompt ---
    summary_tpl = """
    ä½ æ˜¯ä¸€ä½é ‚å°–å¾‹å¸«äº‹å‹™æ‰€çš„è³‡æ·±æ³•å‹™å°ˆå®¶ï¼Œä½ çš„å·¥ä½œæ˜¯ç‚ºä¸‹æ–¹çš„æ³•å¾‹æ¢æ¬¾è£½ä½œä¸€ä»½æ¸…æ™°ã€å®Œæ•´ä¸”å…·é«”çš„ã€Œæ‘˜è¦åˆ†æçŸ©é™£ã€ã€‚

    **ä»»å‹™æŒ‡ç¤º:**
     è«‹åš´æ ¼ç¢ºå®šå›ç­”ä¸è¶…é{st.session_state.max_tokens}tokens,é¿å…å›ç­”ä¸å®Œæ•´ã€‚
    1.  **å®Œæ•´åˆ†æå·®ç•°**:
        * ä»¥ã€Œç¹é«”ä¸­æ–‡ã€é€²è¡Œæ¯”è¼ƒã€‚
        * ä½¿ç”¨ã€Œé»åˆ—å¼ã€(bullet points, e.g., `- ...`) æ¸…æ™°åˆ—å‡ºå…©ä»½æ¢æ¬¾ä¹‹é–“æ‰€æœ‰å…·å¯¦è³ªå½±éŸ¿çš„å·®ç•°é»ã€‚
        * åˆ†æå¿…é ˆå…·é«”ï¼ŒåŒ…å«æœŸé™ã€ç¯„åœã€ç¾©å‹™ç­‰é—œéµäº‹å¯¦çš„æ¯”è¼ƒã€‚

    2.  **æå‡ºå…·é«”ä¿®è¨‚å»ºè­°**:
        * é‡å°ä¸Šè¿°å·®ç•°ï¼Œå°é¢¨éšªè¼ƒé«˜æˆ–å°æˆ‘æ–¹ä¸åˆ©çš„æ¢æ¬¾ï¼Œæå‡ºæ¸…æ™°ã€å¯åŸ·è¡Œçš„ä¿®è¨‚å»ºè­°ã€‚
        * å»ºè­°æ‡‰åŒ…å«ã€Œç‚ºä½•è¦æ”¹ã€ä»¥åŠã€Œå»ºè­°æ”¹æˆä»€éº¼å…§å®¹ã€ã€‚
        * åŒæ¨£ä½¿ç”¨ã€Œé»åˆ—å¼ã€å‘ˆç¾ã€‚

    3.  **åš´æ ¼æ ¼å¼**:
        * ä½ çš„å›ç­”ã€Œåªèƒ½ã€æ˜¯ä¸€è¡Œæ–‡å­—ã€‚
        * å¿…é ˆåš´æ ¼ä½¿ç”¨ '|||' ä½œç‚ºåˆ†éš”ç¬¦ï¼Œåˆ†éš”ã€Œå·®ç•°åˆ†æã€èˆ‡ã€Œä¿®æ”¹å»ºè­°ã€ã€‚
        * åœ¨ '|||' çš„å…©é‚Šï¼Œä½ å¯ä»¥è‡ªç”±ä½¿ç”¨ Markdown çš„é»åˆ—å¼èªæ³• (`- `) èˆ‡æ›è¡Œ (`\n`)ã€‚

    **æ ¼å¼ç¯„ä¾‹:**
    `- å°æ–¹ä¿å¯†æœŸå¾ã€Œåˆç´„çµ‚æ­¢å¾Œã€èµ·ç®—ï¼Œæˆ‘æ–¹ç‚ºã€Œç”Ÿæ•ˆæ—¥èµ·ç®—ã€ã€‚\n- å°æ–¹ä¿å¯†æœŸé•·é”5å¹´ï¼Œæ¯”æˆ‘æ–¹çš„3å¹´æ›´é•·ã€‚ ||| - å»ºè­°å°‡ä¿å¯†æœŸèµ·ç®—é»ä¿®æ”¹ç‚ºã€Œè³‡è¨Šæ­éœ²æ—¥èµ·ã€ï¼Œä»¥ç¢ºä¿å…¬å¹³ã€‚\n- å»ºè­°å°‡æœŸé™ç¸®çŸ­ç‚º3å¹´ï¼Œèˆ‡æˆ‘æ–¹æ¨™æº–ä¸€è‡´ï¼Œé™ä½æˆ‘æ–¹é•·æœŸç¾©å‹™ã€‚`

    ---
    **å¾…åˆ†æè³‡æ–™:**

    **ä¸»é¡Œ**: {topic}

    **æ¢æ¬¾ A (æˆ‘æ–¹ç¯„æœ¬)**:
    ```{clause_A}```

    **æ¢æ¬¾ B (å°æ–¹æ–‡ä»¶)**:
    ```{clause_B}```
    ---

    **è«‹ç«‹å³ç”¢ç”Ÿç¬¦åˆä¸Šè¿°æ‰€æœ‰è¦æ±‚çš„æ‘˜è¦å…§å®¹:**
    """
    summary_chain = PromptTemplate.from_template(summary_tpl) | llm | StrOutputParser()
    
    # --- è©³ç´°å ±å‘Š Prompt ---
    tpl = """
**Role:** You are a seasoned Senior Legal Counsel at a top-tier professional services firm, acting to protect the interests of **"Our Company"**. Your review must be commercially-aware, risk-focused, and provide immediately actionable advice for a non-lawyer project team.

**Objective:** Conduct a detailed preliminary review of a counterparty's contract clause ("Clause B") against Our Company's standard template ("Clause A") on the specific topic of "**{topic}**".

---
**Context 1: Past High-Quality Analysis Examples (for style and depth reference)**
```{approved_examples}```
---
**Context 2: Clause A (Our Company's Standard Template - Normalized to Traditional Chinese)**
```{clause_A}```
---
**Context 3: Clause B (Counterparty's Draft - Normalized to Traditional Chinese)**
```{clause_B}```
---

**Task:** Generate a review report in Markdown. You MUST use the exact headings provided below and address all bullet points within each section.

### 1. æ ¸å¿ƒæ¢æ¬¾æ‘˜è¦ (Clause Summary)
-   **æˆ‘æ–¹ç¯„æœ¬ (Clause A)**: Summarize the core purpose and mechanism of our standard clause in one sentence.
-   **å°æ–¹è‰æ¡ˆ (Clause B)**: Summarize the core purpose and mechanism of their proposed clause in one sentence.

### 2. é—œéµå·®ç•°èˆ‡é¢¨éšªåˆ†æ (Key Differences & Risk Analysis)
-   **å¯¦è³ªå·®ç•°é» (Material Differences)**: Using bullet points, identify *all* significant differences in obligations, timelines, scope, or definitions between the two clauses. Be specific and quantitative (e.g., "30 days vs. 60 days," "includes affiliates vs. does not").
-   **å°æˆ‘æ–¹å•†æ¥­é¢¨éšª (Business Risk to Our Company)**: For each difference identified, explain the potential legal, financial, or operational risk it poses *specifically to Our Company*. Frame it as "This exposes us to the risk of...".

### 3. å…·é«”ä¿®è¨‚èˆ‡è«‡åˆ¤å»ºè­° (Actionable Revision & Negotiation Suggestions)
-   **å»ºè­°ä¿®è¨‚æ–‡å­— (Suggested Redline)**: Provide a direct, copy-pasteable revision to Clause B to mitigate the identified risks and align it closer to our position in Clause A. If no change is needed, state "å»ºè­°æ¥å— (Acceptable as is)".
-   **è«‡åˆ¤åº•ç·šèˆ‡ç­–ç•¥ (Negotiation Points & Bottom Line)**: Briefly state our primary negotiation goal (e.g., "Our main goal is to cap liability at...") and a fallback position if our primary suggestion is rejected.
"""
    analysis_chain = PromptTemplate.from_template(tpl) | llm | StrOutputParser()
    
    detailed_results = {}
    summary_points = []
    progress = st.progress(0, text="AI æ³•å‹™å°ˆå®¶æ­£åœ¨å¯©é–±åˆç´„...")

    mq_template_retriever = MultiQueryRetriever.from_llm(retriever=template_retriever, llm=llm)
    mq_uploaded_retriever = MultiQueryRetriever.from_llm(retriever=uploaded_retriever, llm=llm)
    
    for i, topic in enumerate(review_points):
        display_topic = topic.split(' (')[0].replace('&nbsp;', ' ').strip()
        search_query = topic.replace('&nbsp;', ' ')
        progress.progress((i + 1) / len(review_points), text=f"æ­£åœ¨åˆ†æ: {display_topic}")

        t_docs = mq_template_retriever.get_relevant_documents(search_query)
        u_docs = mq_uploaded_retriever.get_relevant_documents(search_query)
        
        a_text = "ç„¡ç›¸é—œç¯„ä¾‹"
        t_text_original = "\n---\n".join([d.page_content for d in t_docs])
        u_text_original = "\n---\n".join([d.page_content for d in u_docs])
        
        lang_t = get_language(t_text_original, llm)
        lang_u = get_language(u_text_original, llm)
        
        with st.spinner(f"æ­£åœ¨é€²è¡Œèªè¨€æ­£è¦åŒ– ({display_topic})..."):
            t_text_final = translate_to_chinese(t_text_original, llm) if 'en' in lang_t else t_text_original
            u_text_final = translate_to_chinese(u_text_original, llm) if 'en' in lang_u else u_text_original

        if not t_text_final.strip(): t_text_final = "æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›¸é—œæ¢æ¬¾"
        if not u_text_final.strip(): u_text_final = "æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›¸é—œæ¢æ¬¾"
            
        summary_raw = summary_chain.invoke({
            "topic": display_topic,
            "clause_A": t_text_final,
            "clause_B": u_text_final
        })
        try:
            parts = summary_raw.strip().split('|||')
            if len(parts) == 2:
                difference, suggestion = [p.strip() for p in parts]
            else:
                difference, suggestion = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦", "æ ¼å¼éŒ¯èª¤"
        except Exception:
            difference, suggestion = "æ‘˜è¦ç”Ÿæˆå¤±æ•—", "è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤"

        summary_points.append({
            'topic': display_topic,
            'difference': difference,
            'suggestion': suggestion
        })

        report = analysis_chain.invoke({
            "topic": display_topic,
            "approved_examples": a_text,
            "clause_A": t_text_final,
            "clause_B": u_text_final
        })
        detailed_results[topic] = report
        
    progress.empty()
    return {"summary": summary_points, "details": detailed_results}

@st.cache_resource
def load_and_process_pdf_for_faiss(_uploaded_file):
    if not _uploaded_file:
        return None
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(_uploaded_file.getvalue())
        path = tmp.name
    loader = PyPDFLoader(path)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)
    if not split_docs:
        os.remove(path)
        st.info(f"æ–‡ä»¶ '{_uploaded_file.name}' ä¸­æ²’æœ‰å¯è™•ç†çš„æ–‡å­—å…§å®¹ã€‚")
        return None
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(split_docs, embeddings)
    os.remove(path)
    return vs.as_retriever(search_kwargs={'k': 3})

def process_and_store_reference_file(uploaded_file):
    filename = uploaded_file.name
    with st.spinner(f"æ­£åœ¨è™•ç†åƒè€ƒæ–‡ä»¶ '{filename}' ä¸¦è¼‰å…¥è¨˜æ†¶é«”..."):
        retriever = load_and_process_pdf_for_faiss(uploaded_file)
        if retriever:
            st.session_state.reference_retrievers[filename] = retriever
            st.success(f"åƒè€ƒæ–‡ä»¶ '{filename}' å·²æˆåŠŸè¼‰å…¥ï¼")

# --- UI éƒ¨åˆ† (å‰åŠéƒ¨ç¶­æŒä¸è®Š) ---

if "reference_retrievers" not in st.session_state:
    st.session_state.reference_retrievers = {}

st.header("AI åˆç´„åˆå¯©èˆ‡é¢¨éšªåˆ†æ AI-Assisted Contract Preliminary Review and Risk Analysis")

CORE_REVIEW_POINTS = [
    "åˆç´„çš„ä¿å¯†æœŸé™ &nbsp;(Confidentiality Period)",
    "æ©Ÿå¯†è³‡è¨Šçš„å®šç¾©ç¯„åœ &nbsp;(Definition of Confidential Information)",
    "å…è¨±æ­éœ²æ©Ÿå¯†è³‡è¨Šçš„ä¾‹å¤–æƒ…æ³ &nbsp;(Permitted Disclosures)",
    "åˆç´„çš„æº–æ“šæ³•èˆ‡ç®¡è½„æ³•é™¢ &nbsp;(Governing Law and Jurisdiction)",
    "è³‡è¨Šè¿”é‚„æˆ–éŠ·æ¯€çš„ç¾©å‹™ &nbsp;(Return or Destruction of Information)",
    "é‡å°é•ç´„è¡Œç‚ºçš„è£œæ•‘æªæ–½æˆ–è³ å„Ÿæ¢æ¬¾ &nbsp;(Remedies for Breach)",
    "æ™ºæ…§è²¡ç”¢æ¬Šçš„æ­¸å±¬ &nbsp;(Intellectual Property Rights)",
    "é•ç´„é€šçŸ¥èˆ‡æ”¹å–„æœŸé™ &nbsp;(Notice of Breach and Cure Period)"
]
with st.expander("è‡ªè¨‚å¯©æŸ¥é …ç›® Customize Review Parameters", expanded=True):
    cols = st.columns(2)
    for i, point in enumerate(CORE_REVIEW_POINTS):
        with cols[i % 2]:
            st.toggle(point.split(" (")[0], value=True, key=point)
    st.text_area("æ–°å¢å¯©æŸ¥é …ç›®ï¼ˆæ¯è¡Œä¸€å€‹)ï¼š", key="core_points_text", height=100, placeholder="ä¾‹å¦‚ï¼š\nè³ å„Ÿè²¬ä»»ä¸Šé™ (Limitation of Liability)\nåˆç´„çš„å¯è½‰è®“æ€§ (Assignability)")
st.divider()

col1, col2 = st.columns(2)
with col1:
    st.header("æ­¥é©Ÿä¸€ï¼šä¸Šå‚³åƒè€ƒæ–‡ä»¶")
    new_ref_file = st.file_uploader("ä¸Šå‚³ PDF ä½œç‚ºæ–°çš„æ¯”å°åŸºæº–", type="pdf", key="ref_uploader_faiss")
    if st.button("è™•ç†ä¸¦è¼‰å…¥åƒè€ƒæ–‡ä»¶"):
        if new_ref_file:
            process_and_store_reference_file(new_ref_file)
        else:
            st.info("è«‹å…ˆé¸æ“‡ä¸€å€‹åƒè€ƒæ–‡ä»¶ã€‚")
with col2:
    st.header("æ­¥é©ŸäºŒï¼šé¸æ“‡æ¯”å°åŸºæº–")
    processed_files = list(st.session_state.reference_retrievers.keys())
    selected_index = None
    if st.session_state.get("selected_namespace") in processed_files:
        selected_index = processed_files.index(st.session_state.get("selected_namespace"))
    selected = st.selectbox(
        "å¾å·²ä¸Šå‚³çš„åƒè€ƒæ–‡ä»¶ä¸­é¸æ“‡ä¸€ä»½ï¼š",
        options=processed_files,
        index=selected_index,
        placeholder="è«‹é¸æ“‡..."
    )
    if selected is not None: 
        st.session_state.selected_namespace = selected
st.divider()

# --- æ¨¡å‹åƒæ•¸è¨­å®šæ”¹ç‚ºç¨ç«‹çš„æ­¥é©Ÿä¸‰ ---
st.header("æ­¥é©Ÿä¸‰ï¼šè¨­å®š AI åˆ†æåƒæ•¸")
st.session_state.temperature = st.slider(
    "åƒæ•¸æº«åº¦ Temperature", 0.0, 1.0, st.session_state.temperature, 0.05,
    help='æ•¸å€¼è¼ƒä½ï¼Œçµæœæœƒæ›´å…·é«”å’Œä¸€è‡´ï¼›æ•¸å€¼è¼ƒé«˜ï¼Œçµæœæœƒæ›´æœ‰å‰µæ„å’Œå¤šæ¨£æ€§ã€‚'
)
st.session_state.max_tokens = st.slider(
    "æœ€å¤§å­—å…ƒæ•¸ Max Tokens", 256, 4096, st.session_state.max_tokens, 128,
    help='é™åˆ¶å–®æ¬¡ AI å›æ‡‰çš„é•·åº¦ã€‚è¼ƒé•·çš„å ±å‘Šå¯èƒ½éœ€è¦è¼ƒé«˜çš„æ•¸å€¼ã€‚'
)
st.divider()

# --- åŸæ­¥é©Ÿä¸‰æ”¹ç‚ºæ­¥é©Ÿå›› ---
st.header("æ­¥é©Ÿå››ï¼šä¸Šå‚³å¾…å¯©æ–‡ä»¶ä¸¦åŸ·è¡Œåˆ†æ")
selected_namespace = st.session_state.get("selected_namespace")
if not selected_namespace:
    st.info("è«‹åœ¨ä¸Šæ–¹æ­¥é©Ÿä¸€ä¸Šå‚³åƒè€ƒæ–‡ä»¶ï¼Œä¸¦åœ¨æ­¥é©ŸäºŒé¸æ“‡ä¸€ä»½ä½œç‚ºæ¯”å°åŸºæº–ã€‚")
else:
    st.success(f"ç•¶å‰æ¯”å°åŸºæº–ç‚ºï¼š **{selected_namespace}**")

target_file = st.file_uploader("ä¸Šå‚³æ‚¨è¦å¯©æŸ¥çš„åˆç´„æ–‡ä»¶", type="pdf", key="target_uploader")

if target_file: 
    st.session_state.target_file_name = target_file.name

start_button = st.button("é–‹å§‹ AI æ·±åº¦å¯©é–±", type="primary", use_container_width=True, disabled=(not target_file or not selected_namespace))

if start_button:
    with st.spinner("æ­£åœ¨æº–å‚™æ¯”å°ç’°å¢ƒ..."):
        template_retriever = st.session_state.reference_retrievers[selected_namespace]
        uploaded_retriever = load_and_process_pdf_for_faiss(target_file)
        
    if not uploaded_retriever:
        st.info("å¾…å¯©æ–‡ä»¶è™•ç†å¤±æ•—æˆ–å…§å®¹ç‚ºç©ºï¼Œè«‹é‡æ–°ä¸Šå‚³ã€‚")
    else:
        temp = st.session_state.temperature
        max_tok = st.session_state.max_tokens
        
        active_review_points = [p for p in CORE_REVIEW_POINTS if st.session_state.get(p, True)]
        custom_points = [line.strip() for line in st.session_state.get("core_points_text", "").split('\n') if line.strip()]
        final_review_points = active_review_points + custom_points

        if not final_review_points:
            st.info("è«‹è‡³å°‘é¸æ“‡æˆ–æ–°å¢ä¸€å€‹å¯©æŸ¥é …ç›®ã€‚")
        else:
            st.session_state.comparison_results = run_comparison(template_retriever, uploaded_retriever, final_review_points, temp, max_tok)
            st.rerun()

# --- æ•´åˆç¬¬äº”é çš„å ±å‘Šé¡¯ç¤ºèˆ‡å„²å­˜åŠŸèƒ½ ---
if st.session_state.get("comparison_results"):
    st.balloons()
    st.header("âœ… AI æ·±åº¦å¯©é–±å ±å‘Šå·²å®Œæˆ")
    st.info("æ‚¨å¯ä»¥æª¢è¦–ä¸‹æ–¹çš„æ‘˜è¦èˆ‡å ±å‘Šï¼Œè‹¥æ‚¨èªç‚ºé€™ä»½å ±å‘Šå“è³ªå„ªè‰¯ï¼Œå¯å°‡å…¶æ­¸æª”ç”¨æ–¼ AI å†å­¸ç¿’ã€‚")
    st.divider()

    # --- å ±å‘Šé è¦½ ---
    st.subheader("é¢¨éšªæ‘˜è¦ç¸½è¦½")
    
    summary_data = st.session_state.comparison_results['summary']
    details_data = st.session_state.comparison_results['details']

    # å»ºç«‹ä¸€å€‹å®Œæ•´çš„ Markdown å­—ä¸²ç”¨æ–¼å¾ŒçºŒå„²å­˜
    full_report_md = "# AI åˆç´„å¯©é–±å ±å‘Š\n\n"
    
    # æ‘˜è¦è¡¨æ ¼
    summary_table_md = f"| **é …ç›®** | **ä¸»è¦å·®ç•°** | **æ ¸å¿ƒä¿®æ”¹å»ºè­°** |\n"
    summary_table_md += "|:---|:---|:---|\n"
    for item in summary_data:
        # ç‚ºäº†é¡¯ç¤ºå’Œå„²å­˜ï¼Œæˆ‘å€‘éœ€è¦è™•ç†æ›è¡Œ
        difference_display = item['difference'].replace('\n', '<br>')
        suggestion_display = item['suggestion'].replace('\n', '<br>')
        summary_table_md += f"| {item['topic']} | {difference_display} | {suggestion_display} |\n"
    
    st.markdown(summary_table_md, unsafe_allow_html=True)
    full_report_md += "## é¢¨éšªæ‘˜è¦ç¸½è¦½\n\n" + summary_table_md.replace('<br>', '\n') + "\n\n"
    st.divider()
    
    # è©³ç´°å ±å‘Š
    st.subheader("é€é …å¯©é–±å ±å‘Š")
    full_report_md += "## é€é …å¯©é–±å ±å‘Š\n\n"
    for topic, report_md in details_data.items():
        with st.expander(f"**å¯©æŸ¥é …ç›®ï¼š{topic.split(' (')[0]}**", expanded=True):
            st.markdown(report_md, unsafe_allow_html=True)
        full_report_md += f"### å¯©æŸ¥é …ç›®ï¼š{topic.split(' (')[0]}\n\n{report_md}\n\n---\n\n"
        
    st.divider()
    
    # --- [æ–°å¢] æ­¸æª”èˆ‡å­¸ç¿’åŠŸèƒ½ ---
    st.subheader("ğŸ§  åˆ†ææ­¸æª”èˆ‡ AI å†å­¸ç¿’")
    st.markdown("è‹¥æ‚¨èªå¯é€™ä»½å ±å‘Šçš„åˆ†æå“è³ªï¼Œå¯ä»¥é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œç³»çµ±æœƒå°‡å…¶æ­¸æª”è‡³ Amazon S3ï¼Œä¸¦å°‡å…¶å…§å®¹ä½œç‚ºä¸€å€‹å®Œæ•´çš„ã€Œå„ªè‰¯ç¯„ä¾‹ã€é¤µçµ¦ AI é€²è¡Œå­¸ç¿’ã€‚")

    if st.button("âœ… æˆ‘èªå¯é€™ä»½å ±å‘Šçš„å“è³ªï¼Œæ­¸æª”è‡³é›²ç«¯ä¸¦ç”¨æ–¼ AI å­¸ç¿’", type="primary", use_container_width=True):
        # æº–å‚™è¦å„²å­˜çš„å…§å®¹å’Œæª”å
        template_name = st.session_state.get("selected_namespace", "template").replace('.pdf', '')
        target_name = st.session_state.get("target_file_name", "target").replace('.pdf', '')
        timestamp = datetime.now().strftime('%Y%m%d')
        
        storage_filename = f"Approved_Report_{template_name}_vs_{target_name}_{timestamp}.md"
        
        # 1. å‘¼å« S3 çš„ä¸Šå‚³å‡½å¼
        upload_success = storage.upload_report_to_storage(full_report_md, filename=storage_filename)

        # 2. ç¢ºä¿é›²ç«¯ä¸Šå‚³æˆåŠŸå¾Œæ‰é€²è¡Œå­¸ç¿’
        if upload_success:
            try:
                with st.spinner(f"æ­£åœ¨å°‡å ±å‘ŠçŸ¥è­˜è½‰åŒ–ç‚º AI çš„é•·æœŸè¨˜æ†¶..."):
                    # æº–å‚™é¤µçµ¦ AI çš„æ–‡å­—å…§å®¹ï¼ŒåŠ ä¸Šæ¨™é¡Œä»¥æä¾›ä¸Šä¸‹æ–‡
                    learning_content = f"ã€å„ªè‰¯åˆ†ææ¡ˆä¾‹ï¼šåˆç´„å¯©é–±å ±å‘Š - {template_name} vs {target_name}ã€‘\n\n{full_report_md}"
                    
                    # å¯«å…¥æš«å­˜æª”
                    with tempfile.NamedTemporaryFile(delete=False, mode="w+", encoding="utf-8", suffix=".txt") as tmp_file:
                        tmp_file.write(learning_content)
                        tmp_file_path = tmp_file.name
                    
                    # ä½¿ç”¨ TextLoader è¼‰å…¥ä¸¦ä¸Šå‚³è‡³ Pinecone
                    loader = TextLoader(tmp_file_path)
                    documents = loader.load()
                    ingest_docs_to_pinecone(documents, INDEX_NAME, LEARNING_NAMESPACE)
                    os.remove(tmp_file_path)
                    
                    st.success(f"AI å·²æˆåŠŸå­¸ç¿’æ­¤ä»½å ±å‘Šçš„åˆ†ææ¨¡å¼ï¼")
                    
                    # è™•ç†å®Œæˆå¾Œï¼Œæ¸…ç©º session_state ä¸¦é¡¯ç¤ºæˆåŠŸè¨Šæ¯
                    st.session_state.comparison_results = None
                    st.header("è™•ç†å®Œæˆï¼å„ªè³ªå ±å‘Šå·²æˆåŠŸæ­¸æª”ä¸¦ç”¨æ–¼ AI å†å­¸ç¿’ã€‚")
                    st.info("é é¢å³å°‡åˆ·æ–°...")
                    st.rerun()

            except Exception as e:
                st.error(f"åœ¨ AI å­¸ç¿’éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
