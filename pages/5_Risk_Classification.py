import os
import re
import io
import json
import time
import textwrap
from typing import List, Literal, Optional

import streamlit as st
from pydantic import BaseModel, Field, ValidationError
from PyPDF2 import PdfReader
from openai import OpenAI
from dotenv import load_dotenv
import fitz

# å‡è¨­æ‚¨çš„ text_splitter æ¨¡çµ„ä½æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„
from text_splitter import smart_split

# ç‚ºäº†èªè¨€åµæ¸¬èˆ‡ç²¾æº–æå–ï¼Œå°å…¥ LangChain èˆ‡ spaCy
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
import spacy

import Risk_Knowledge

load_dotenv()
# ---------------- UI Configuration ----------------
st.set_page_config(page_title="Contract Risk Classifier", layout="wide")
st.logo("logo.png")

# ---------------- Configuration ----------------
MODEL_NAME = "gpt-4o"
EXTRACTION_MODEL_NAME = "gpt-4o-mini"

# ---------------- Pydantic Schema ----------------
class ClauseRisk(BaseModel):
    clause: str
    risk: Literal["HIGH", "MEDIUM", "NOTICEABLE"]
    risk_sentence: Optional[str] = Field(None)
    reason: str
    tags: List[str] = Field(default_factory=list)

RISK_RUBRIC = Risk_Knowledge.get_risk_rubric_string()

# ---------------- Prompts ----------------
SYSTEM_PROMPT_STAGE1 = f"""
You are a meticulous contract risk analyst. Your task is to analyze a clause and classify its risk.
**Analysis Steps:**
1.  **Analyze**: Identify core legal and commercial implications.
2.  **Consult Rubric**: Compare against the `Risk Classification Rubric`.
3.  **Determine Risk Level**:
    - If it matches a "High" or "Medium" risk description, classify it as **"HIGH"** or **"MEDIUM"**.
    - If the clause does **NOT** match High/Medium risks but matches a "Noticeable Clause" description (e.g., standard governing law, confidentiality period), classify it as **"NOTICEABLE"**.
4.  **Formulate Reason**: Write a concise explanation in Traditional Chinese (ç¹é«”ä¸­æ–‡) for your classification.
5.  **Extract Keywords**: Identify 2-4 keywords.
6.  **Construct JSON**: Assemble into a SINGLE JSON object.
**Risk Classification Rubric:**
---
{RISK_RUBRIC}
---
**Output Format Rules (Strictly Enforced):**
- Output MUST be a single, valid JSON object.
- Keys must be EXACTLY: `risk`, `reason`, `tags`.
- `risk` must be "HIGH", "MEDIUM", or "NOTICEABLE".
- Do NOT return the original `clause`.
"""

SYSTEM_PROMPT_STAGE2 = """
You are a legal text analysis assistant. Given a full clause and a reason for its risk, extract the **exact, single sentence (or at most two)** that is the primary source of the risk.
Respond with ONLY the extracted sentence(s). No explanation, no preamble, no quotes.
"""

# ---------------- Helpers ----------------
def extract_text_from_pdf(file_bytes_io: io.BytesIO) -> str:
    reader = PdfReader(file_bytes_io)
    return "\n".join((page.extract_text() or "") for page in reader.pages)

@st.cache_data(show_spinner=False)
def get_language(text_snippet: str) -> str:
    if not text_snippet.strip(): return "en"
    try:
        llm = ChatOpenAI(model_name=EXTRACTION_MODEL_NAME, temperature=0)
        prompt = PromptTemplate.from_template("Detect the primary language (ISO 639-1 code, 'en' or 'zh') of this text: ```{text}```")
        chain = prompt | llm | StrOutputParser()
        lang_code = chain.invoke({"text": text_snippet[:500]}).lower()
        return 'zh' if 'zh' in lang_code else 'en'
    except Exception:
        return "en"

def sanitize_risk_data(risk_data: dict) -> dict:
    if "tags" not in risk_data or not isinstance(risk_data.get("tags"), list):
        risk_data["tags"] = []
    raw_risk = str(risk_data.get("risk", "MEDIUM")).upper()
    if "HIGH" in raw_risk:
        risk_data["risk"] = "HIGH"
    elif "MEDIUM" in raw_risk:
        risk_data["risk"] = "MEDIUM"
    elif "NOTICEABLE" in raw_risk or "LOW" in raw_risk or "STANDARD" in raw_risk:
        risk_data["risk"] = "NOTICEABLE"
    else:
        risk_data["risk"] = "MEDIUM"
        risk_data["tags"].append("risk_parse_failed")
    return risk_data

def classify_clause(client: OpenAI, clause: str) -> ClauseRisk:
    resp_stage1 = client.chat.completions.create(
        model=MODEL_NAME, temperature=0.1,
        messages=[{"role": "system", "content": SYSTEM_PROMPT_STAGE1}, {"role": "user", "content": clause}],
        response_format={"type": "json_object"},
    )
    if not resp_stage1.choices or not resp_stage1.choices[0].message or not resp_stage1.choices[0].message.content:
        raise ValueError("AI model returned an empty or invalid response in Stage 1.")
    content_str = resp_stage1.choices[0].message.content
    try:
        match = re.search(r"\{.*\}", content_str, re.DOTALL)
        if match:
            risk_data = json.loads(match.group(0))
        else:
            risk_data = json.loads(content_str)
    except json.JSONDecodeError:
        raise ValueError(f"Failed to parse JSON from model response: {content_str}")
    risk_data = sanitize_risk_data(risk_data)
    risk_data["clause"] = clause
    risk_data.setdefault("risk_sentence", None)
    if risk_data.get("risk") in ["HIGH", "MEDIUM"]:
        user_prompt_stage2 = f"Full Clause:\n```\n{clause}\n```\n\nReason for Risk:\n{risk_data.get('reason', '')}"
        resp_stage2 = client.chat.completions.create(
            model=EXTRACTION_MODEL_NAME, temperature=0,
            messages=[{"role": "system", "content": SYSTEM_PROMPT_STAGE2}, {"role": "user", "content": user_prompt_stage2}],
        )
        if resp_stage2.choices and resp_stage2.choices[0].message and resp_stage2.choices[0].message.content:
            risk_sentence = resp_stage2.choices[0].message.content.strip()
            if risk_sentence and risk_sentence in clause:
                risk_data["risk_sentence"] = risk_sentence
    return ClauseRisk.model_validate(risk_data)

def classify_batch(clauses: List[str]) -> List[ClauseRisk]:
    client = OpenAI()
    out: List[ClauseRisk] = []
    total = max(len(clauses), 1)
    progress = st.progress(0, text="æ­£åœ¨åˆå§‹åŒ– AI åˆ†æ...")
    for i, c in enumerate(clauses, start=1):
        progress.progress(min(i / total, 1.0), text=f"æ­£åœ¨åˆ†æç¬¬ {i}/{total} æ¢æ¬¾...")
        try:
            out.append(classify_clause(client, c))
        except (ValueError, ValidationError) as e:
            out.append(ClauseRisk(clause=c, risk="NOTICEABLE", reason=f"æ¨¡å‹åˆ†ææˆ–é©—è­‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)[:100]}", tags=["error", "parsing_failed"]))
        except Exception as e:
            out.append(ClauseRisk(clause=c, risk="NOTICEABLE", reason=f"ç™¼ç”Ÿæœªé æœŸçš„ç³»çµ±éŒ¯èª¤: {str(e)[:100]}", tags=["error", "system_error"]))
        time.sleep(0.05)
    progress.empty()
    return out

def _normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def _candidate_snippets(text: str, min_len: int = 20, max_len: int = 120) -> List[str]:
    t = _normalize_spaces(text)
    if not t: return []
    if len(t) <= max_len: return [t]
    sentences = re.split(r'(?<=[.?!;:])\s+', t)
    valid_sentences = [s.strip() for s in sentences if len(s.strip()) >= min_len]
    if valid_sentences:
        return [s[:max_len] for s in valid_sentences]
    return [t[:max_len], t[-max_len:]]

def _inset(rect, margin: float) -> "fitz.Rect":
    return fitz.Rect(rect.x0 + margin, rect.y0 + margin, rect.x1 - margin, rect.y1 - margin)

def build_highlighted_pdf(src_pdf_bytes: bytes, items: List[ClauseRisk], include_resolved: bool = False, resolved_idx: set = None) -> bytes:
    if resolved_idx is None: resolved_idx = set()
    doc = fitz.open(stream=src_pdf_bytes, filetype="pdf")
    not_found = []
    for idx, item in enumerate(items):
        if (not include_resolved and idx in resolved_idx) or item.risk not in ["HIGH", "MEDIUM"]:
            continue
        text_to_search = item.risk_sentence if item.risk_sentence else item.clause
        snippets = _candidate_snippets(text_to_search)
        found_any = False
        for page in doc:
            for snip in snippets:
                if not isinstance(snip, str) or len(snip) < 5: continue
                rects = page.search_for(snip, quads=True)
                if rects:
                    annot = page.add_highlight_annot(rects)
                    annot.set_colors(stroke=(1, 0, 0) if item.risk == "HIGH" else (1, 0.55, 0))
                    annot.set_info(content=f"[{item.risk}] {item.reason}")
                    annot.update()
                    found_any = True
        if not found_any:
            not_found.append(item)
    if not_found:
        summary_page = doc.new_page()
        header = "Clauses Not Found For Highlighting"
        text_blocks = [header, ""]
        for miss in not_found:
            text_blocks.append(f"- {miss.risk} Â· {', '.join(miss.tags) or 'untagged'}")
            text_blocks.append(f"  Reason: {miss.reason}")
            text_blocks.append("  " + textwrap.shorten(_normalize_spaces(miss.clause), width=220))
            text_blocks.append("")
        inner_rect = _inset(summary_page.rect, 36)
        summary_page.insert_textbox(inner_rect, "\n".join(text_blocks), fontsize=10, align=0)
    out = io.BytesIO()
    doc.save(out, deflate=True, garbage=4)
    doc.close()
    return out.getvalue()

# ---------------- App Header & Controls ----------------
st.header("åˆç´„é¢¨éšªè©•é‘‘ Contract Risk Classifier")
st.markdown("ä¸Šå‚³åˆç´„ PDFï¼ŒAI å°‡è‡ªå‹•æ¨™ç¤º**é«˜/ä¸­**é¢¨éšªæ¢æ¬¾ï¼Œ**ç²¾æº–å®šä½é¢¨éšªå¥å­**ï¼Œä¸¦é™„ä¸Šåˆ†æèªªæ˜ã€‚")
if "results" not in st.session_state: st.session_state.results = []
if "resolved" not in st.session_state: st.session_state.resolved = set()
if "source_pdf_bytes" not in st.session_state: st.session_state.source_pdf_bytes = None
left, right = st.columns([2, 1])
with left:
    uploaded = st.file_uploader("ä¸Šå‚³æ‚¨çš„åˆç´„ (.pdf)", type=["pdf"], key="contract_pdf")
    process_clicked = st.button("è™•ç†ä¸¦é€²è¡Œ AI é¢¨éšªåˆ†æ", type="primary", use_container_width=True, disabled=uploaded is None)
with right:
    cap = st.number_input("æœ€å¤§å¯åˆ†ææ¢æ¬¾æ•¸", min_value=5, max_value=50, value=20, step=5)
    split_method = st.selectbox("æ–‡å­—åˆ‡å‰²æ–¹å¼", options=["semantic", "regex", "recursive"], index=0)
    MODEL_NAME = st.selectbox("åˆ†ææ¨¡å‹ (é€²éš)", options=["gpt-4o", "gpt-4-turbo", "gpt-5", "gpt-5-pro"], index=0)

# ---------------- Process PDF ----------------
if process_clicked:
    if not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY æœªè¨­å®šã€‚")
        st.stop()
    if uploaded is None:
        st.warning("è«‹å…ˆä¸Šå‚³ PDFã€‚")
        st.stop()
    
    try:
        pdf_bytes = uploaded.getvalue()
        st.session_state.source_pdf_bytes = pdf_bytes
        text = extract_text_from_pdf(io.BytesIO(pdf_bytes))
    except Exception as e:
        st.error(f"è®€å–æˆ–è§£æ PDF æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        st.stop()

    lang = get_language(text)
    st.caption(f"åµæ¸¬åˆ°æ–‡ä»¶ä¸»è¦èªè¨€ç‚º: **{'ä¸­æ–‡ (zh)' if lang == 'zh' else 'English (en)'}**")
    clauses = smart_split(text, method=split_method)[:cap]
    st.caption(f"ä½¿ç”¨ '{split_method}' æ–¹å¼åˆ‡å‰²å‡º {len(clauses)} å€‹æ¢æ¬¾å€å¡Šã€‚")
    if not clauses:
        st.warning("æœªèƒ½åœ¨æ–‡ä»¶ä¸­åˆ‡å‰²å‡ºæœ‰æ•ˆçš„æ¢æ¬¾ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æˆ–å˜—è©¦å…¶ä»–åˆ‡å‰²æ–¹å¼ã€‚")
        st.stop()
    
    all_results = classify_batch(clauses)
    
    # âœ… [æ ¸å¿ƒä¿®æ”¹] åœ¨æ­¤è™•éæ¿¾æ‰ NOTICEABLE çš„é …ç›®
    high_medium_results = [r for r in all_results if r.risk in ["HIGH", "MEDIUM"]]
    noticeable_count = len(all_results) - len(high_medium_results)
    
    st.session_state.results = high_medium_results
    st.session_state.resolved = set()

    # å„²å­˜ä¸€å€‹è¨Šæ¯ï¼Œä»¥ä¾¿åœ¨é é¢åˆ·æ–°å¾Œé¡¯ç¤º
    st.session_state.last_run_message = f"åˆ†æå®Œæˆï¼å·²è­˜åˆ¥å‡º {len(high_medium_results)} å€‹é«˜/ä¸­é¢¨éšªé …ç›®ã€‚"
    if noticeable_count > 0:
        st.session_state.last_run_message += f" (å·²è‡ªå‹•éæ¿¾ {noticeable_count} å€‹ä½é¢¨éšªé …ç›®)"

    st.rerun()

# ---------------- Render Results ----------------
# åœ¨é é¢é ‚éƒ¨é¡¯ç¤ºä¸Šæ¬¡é‹è¡Œçš„çµæœè¨Šæ¯
if "last_run_message" in st.session_state:
    st.success(st.session_state.last_run_message)
    del st.session_state.last_run_message # é¡¯ç¤ºä¸€æ¬¡å¾Œå°±åˆªé™¤

if st.session_state.results:
    results: List[ClauseRisk] = st.session_state.results
    resolved: set = st.session_state.resolved
    def mark_resolved(idx: int): st.session_state.resolved.add(idx)
    def undo_resolved(idx: int): st.session_state.resolved.discard(idx)
    active_indices = [i for i in range(len(results)) if i not in resolved]
    active_results = [results[i] for i in active_indices]
    
    # âœ… [æ ¸å¿ƒä¿®æ”¹] ç°¡åŒ–è¨ˆæ•¸å™¨ï¼Œä¸å†éœ€è¦è¨ˆç®— NOTICEABLE
    counts = {"HIGH": sum(1 for r in active_results if r.risk == "HIGH"),
              "MEDIUM": sum(1 for r in active_results if r.risk == "MEDIUM")}
              
    st.divider()
    st.subheader("é¢¨éšªæ‘˜è¦ Summary")
    # âœ… [æ ¸å¿ƒä¿®æ”¹] æ›´æ–°æ‘˜è¦é¡¯ç¤ºï¼Œç§»é™¤ NOTICEABLE
    st.write(f"ğŸ”´ **é«˜é¢¨éšª: {counts['HIGH']}** Â· ğŸŸ  **ä¸­é¢¨éšª: {counts['MEDIUM']}** Â· âœ… **å·²è§£æ±º: {len(resolved)}**")
    
    show_resolved = st.checkbox("é¡¯ç¤ºå·²è§£æ±ºé …ç›®", value=False)
    
    # âœ… [æ ¸å¿ƒä¿®æ”¹] ç°¡åŒ– badge_map
    badge_map = {"HIGH": "ğŸ”´ é«˜é¢¨éšª HIGH RISK", "MEDIUM": "ğŸŸ  ä¸­é¢¨éšª MEDIUM RISK"}

    def render_card(idx: int, item: ClauseRisk, is_resolved: bool):
        with st.container(border=True):
            left_col, right_col = st.columns([8, 2])
            with left_col:
                # ç”±æ–¼å·²éæ¿¾ï¼Œæ­¤è™• get çš„é è¨­å€¼ä¸å†é‡è¦ï¼Œä½†ä¿ç•™ä»¥é˜²è¬ä¸€
                status = "âœ… å·²è§£æ±º RESOLVED" if is_resolved else badge_map.get(item.risk, "ğŸŸ  ä¸­é¢¨éšª MEDIUM RISK")
                st.markdown(f"**{status}** Â· *Tags: {', '.join(item.tags) or 'ç„¡'}*")
                if item.risk_sentence and not is_resolved:
                    st.markdown("##### ğŸ”‘ é¢¨éšªæ ¹æºå¥ (Risk Root-Cause Sentence)")
                    st.markdown(f"> {item.risk_sentence}")
                    st.markdown("---")
                with st.expander("æª¢è¦–å®Œæ•´æ¢æ¬¾ä¸Šä¸‹æ–‡ (View Full Clause Context)"):
                    st.text_area("Clause Text", value=item.clause, height=150, disabled=True, key=f"clause_text_{idx}")
                st.caption(f"**AI åˆ†æèˆ‡ç†ç”±:** {item.reason}")
            with right_col:
                if not is_resolved:
                    st.button("Resolved", key=f"resolve_btn_{idx}", help="å°‡æ­¤é …ç›®æ¨™ç¤ºç‚ºå·²è§£æ±º", on_click=mark_resolved, args=(idx,), use_container_width=True)
                else:
                    st.button("Undo", key=f"undo_btn_{idx}", help="å°‡æ­¤é …ç›®ç§»å›å¾…è™•ç†æ¸…å–®", on_click=undo_resolved, args=(idx,), use_container_width=True)

    for i in active_indices:
        render_card(i, results[i], is_resolved=False)
    if show_resolved and resolved:
        st.markdown("### Resolved Items")
        for i in sorted(list(resolved)):
            render_card(i, results[i], is_resolved=True)
    
    st.divider()
    st.subheader("åŒ¯å‡ºé‡é»æ¨™è¨˜ Export with Highlights (PDF)")
    if st.session_state.source_pdf_bytes:
        include_resolved_pdf = st.toggle("åœ¨ PDF ä¸­åŒ…å«å·²è§£æ±ºé …ç›®", value=False)
        try:
            pdf_bytes = build_highlighted_pdf(
                st.session_state.source_pdf_bytes,
                items=results,
                include_resolved=include_resolved_pdf,
                resolved_idx=resolved
            )
            st.download_button(
                "åŒ¯å‡ºå«é‡é»æ¨™è¨»çš„ PDF",
                data=pdf_bytes, file_name="contract_highlighted.pdf",
                mime="application/pdf", use_container_width=True,
            )
        except Exception as e:
            st.error(f"ç”¢ç”Ÿ PDF æ¨™ç¤ºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
