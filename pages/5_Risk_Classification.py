import os
import re
import io
import json
import time
import textwrap
from typing import List, Literal, Optional

import streamlit as st
from pydantic import BaseModel, Field, ValidationError
#from PyPDF2 import PdfReader
from openai import OpenAI
from dotenv import load_dotenv
import fitz

# å‡è¨­æ‚¨çš„ text_splitter æ¨¡çµ„ä½æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„
from text_splitter import smart_split

# ç‚ºäº†èªè¨€åµæ¸¬èˆ‡ç²¾æº–æå–ï¼Œå°å…¥ LangChain èˆ‡ spaCy
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
import spacy

import Risk_Knowledge

load_dotenv()
# ---------------- UI Configuration ----------------
st.set_page_config(page_title="Contract Risk Classifier", layout="wide")
st.logo("logo.png")

# ---------------- Configuration ----------------
MODEL_NAME = "gpt-4o"
EXTRACTION_MODEL_NAME = "gpt-4o-mini"

# ---------------- Pydantic Schema ----------------
class ClauseRisk(BaseModel):
    """è³‡æ–™çµæ§‹ï¼ŒåŒ…å« risk, reason, å’Œ suggestion æ¬„ä½"""
    clause: str
    risk: Literal["HIGH", "MEDIUM", "NOTICEABLE"]
    risk_sentence: Optional[str] = Field(None)
    reason: str
    suggestion: Optional[str] = Field(None, description="A complete, revised version of the clause text to mitigate risk.")
    tags: List[str] = Field(default_factory=list)

RISK_RUBRIC = Risk_Knowledge.get_risk_rubric_string()

# ---------------- Prompts (æ ¸å¿ƒä¿®æ”¹) ----------------
# ç”¨æ›´æ˜ç¢ºçš„æŒ‡ä»¤ï¼Œç¢ºä¿ AI æä¾›å®Œæ•´çš„ä¿®è¨‚æ¢æ–‡
SYSTEM_PROMPT_STAGE1 = f"""
You are a senior legal counsel specializing in contract review. Your sole task is to analyze a **single given contract clause**, classify its risk level, and provide a complete, rewritten version of the clause that is ready for use.

**Analysis Steps (Follow Strictly):**
1.  **Analyze ONLY the Provided Text**: Your entire analysis **MUST** be based **exclusively** on the text of the clause given to you. **It is strictly forbidden to invent, assume, or refer to other clause numbers or topics** (e.g., 'Article 5' or 'indemnification') unless they are explicitly written in the provided text. Your analysis must directly correspond to the content of the input clause.
2.  **Consult Rubric**: Compare the clause against the `Risk Classification Rubric` provided below.
3.  **Determine Risk Level**:
    - Classify as **"HIGH"** or **"MEDIUM"** if it matches a corresponding risk description in the rubric.
    - Classify as **"NOTICEABLE"** only if it does NOT match High/Medium risks but pertains to standard matters (e.g., governing law, confidentiality period).
4.  **Formulate Reason**: In Traditional Chinese (ç¹é«”ä¸­æ–‡), write a concise, clear explanation for your risk classification.
5.  **Provide Full Revision (Crucial Task)**:
    - Provide the revised clause in the **same language** as the original clause.
    - For any "HIGH" or "MEDIUM" risk clause, you **MUST** provide a **complete, standalone, and rewritten version of the clause**. This rewritten clause should mitigate all identified risks and be ready to replace the original text. It is not just a comment, but the full revised text.
    - For "NOTICEABLE" clauses or if the original text is already acceptable, respond with the exact phrase "ç„¡éœ€ä¿®æ”¹".
6.  **Extract Keywords**: Identify 2-4 relevant keywords from the clause.
7.  **Construct JSON**: Assemble your entire analysis into a SINGLE, valid JSON object.

**Risk Classification Rubric:**
---
{RISK_RUBRIC}
---

**Output Format Rules (Strictly Enforced):**
- Your output **MUST** be a single, valid JSON object.
- The JSON keys must be **EXACTLY**: `risk`, `reason`, `suggestion`, `tags`.
- `risk` must be one of "HIGH", "MEDIUM", or "NOTICEABLE".
- `reason` must be in Traditional Chinese.
- `suggestion` **MUST** contain the full, revised clause text in Traditional Chinese, or the exact phrase "ç„¡éœ€ä¿®æ”¹".
- Do **NOT** include the original `clause` in your JSON response.
"""


SYSTEM_PROMPT_STAGE2 = """
You are a legal text analysis assistant. Given a full clause and a reason for its risk, extract the **exact, single sentence (or at most two)** that is the primary source of the risk.
Respond with ONLY the extracted sentence(s). No explanation, no preamble, no quotes.
"""

# ---------------- Helpers (ç„¡éœ€ä¿®æ”¹) ----------------
# pages/5_Risk_Classification.py

def extract_text_from_pdf(file_bytes_io: io.BytesIO) -> str:
    """
    [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ PyMuPDF (fitz) é€²è¡Œæ–‡å­—æå–ã€‚
    é€™ç¢ºä¿äº†æå–æ–‡å­—èˆ‡å¾ŒçºŒæ¨™è¨»æœå°‹æ‰€ç”¨çš„å¼•æ“æ˜¯åŒä¸€å€‹ï¼Œ
    å¾è€Œè§£æ±ºäº†å› å‡½å¼åº«å·®ç•°å°è‡´çš„ä¸­æ–‡åŒ¹é…å¤±æ•—å•é¡Œã€‚
    """
    doc = fitz.open(stream=file_bytes_io, filetype="pdf")
    full_text = []
    for page in doc:
        full_text.append(page.get_text() or "")
    doc.close()
    return "\n".join(full_text)

@st.cache_data(show_spinner=False)
def get_language(text_snippet: str) -> str:
    if not text_snippet.strip(): return "en"
    try:
        llm = ChatOpenAI(model_name=EXTRACTION_MODEL_NAME, temperature=0)
        prompt = PromptTemplate.from_template("Detect the primary language (ISO 639-1 code, 'en' or 'zh') of this text: ```{text}```")
        chain = prompt | llm | StrOutputParser()
        lang_code = chain.invoke({"text": text_snippet[:500]}).lower()
        return 'zh' if 'zh' in lang_code else 'en'
    except Exception:
        return "en"

def sanitize_risk_data(risk_data: dict) -> dict:
    if "tags" not in risk_data or not isinstance(risk_data.get("tags"), list):
        risk_data["tags"] = []
    raw_risk = str(risk_data.get("risk", "MEDIUM")).upper()
    if "HIGH" in raw_risk:
        risk_data["risk"] = "HIGH"
    elif "MEDIUM" in raw_risk:
        risk_data["risk"] = "MEDIUM"
    elif "NOTICEABLE" in raw_risk or "LOW" in raw_risk or "STANDARD" in raw_risk:
        risk_data["risk"] = "NOTICEABLE"
    else:
        risk_data["risk"] = "MEDIUM"
        risk_data["tags"].append("risk_parse_failed")
    return risk_data

def classify_clause(client: OpenAI, clause: str) -> ClauseRisk:
    resp_stage1 = client.chat.completions.create(
        model=MODEL_NAME, temperature=0.1,
        messages=[{"role": "system", "content": SYSTEM_PROMPT_STAGE1}, {"role": "user", "content": clause}],
        response_format={"type": "json_object"},
    )
    if not resp_stage1.choices or not resp_stage1.choices[0].message or not resp_stage1.choices[0].message.content:
        raise ValueError("AI model returned an empty or invalid response in Stage 1.")
    content_str = resp_stage1.choices[0].message.content
    try:
        match = re.search(r"\{.*\}", content_str, re.DOTALL)
        if match:
            risk_data = json.loads(match.group(0))
        else:
            risk_data = json.loads(content_str)
    except json.JSONDecodeError:
        raise ValueError(f"Failed to parse JSON from model response: {content_str}")

    risk_data = sanitize_risk_data(risk_data)
    risk_data["clause"] = clause
    risk_data.setdefault("risk_sentence", None)
    risk_data.setdefault("suggestion", "ç„¡æ³•ç”Ÿæˆå»ºè­°ã€‚")

    if risk_data.get("risk") in ["HIGH", "MEDIUM"]:
        user_prompt_stage2 = f"Full Clause:\n```\n{clause}\n```\n\nReason for Risk:\n{risk_data.get('reason', '')}"
        resp_stage2 = client.chat.completions.create(
            model=EXTRACTION_MODEL_NAME, temperature=0,
            messages=[{"role": "system", "content": SYSTEM_PROMPT_STAGE2}, {"role": "user", "content": user_prompt_stage2}],
        )
        if resp_stage2.choices and resp_stage2.choices[0].message and resp_stage2.choices[0].message.content:
            risk_sentence = resp_stage2.choices[0].message.content.strip()
            if risk_sentence and risk_sentence in clause:
                risk_data["risk_sentence"] = risk_sentence
    return ClauseRisk.model_validate(risk_data)

def classify_batch(clauses: List[str]) -> List[ClauseRisk]:
    client = OpenAI()
    out: List[ClauseRisk] = []
    total = max(len(clauses), 1)
    progress = st.progress(0, text="æ­£åœ¨åˆå§‹åŒ– AI åˆ†æ...")
    for i, c in enumerate(clauses, start=1):
        progress.progress(min(i / total, 1.0), text=f"æ­£åœ¨åˆ†æç¬¬ {i}/{total} æ¢æ¬¾...")
        try:
            out.append(classify_clause(client, c))
        except (ValueError, ValidationError) as e:
            out.append(ClauseRisk(clause=c, risk="NOTICEABLE", reason=f"æ¨¡å‹åˆ†ææˆ–é©—è­‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)[:100]}", suggestion="N/A", tags=["error", "parsing_failed"]))
        except Exception as e:
            out.append(ClauseRisk(clause=c, risk="NOTICEABLE", reason=f"ç™¼ç”Ÿæœªé æœŸçš„ç³»çµ±éŒ¯èª¤: {str(e)[:100]}", suggestion="N/A", tags=["error", "system_error"]))
        time.sleep(0.05)
    progress.empty()
    return out

def _normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

# pages/5_Risk_Classification.py

def _candidate_snippets(text: str, min_len: int = 15, max_len: int = 100) -> List[str]:
    """
    [æ ¸å¿ƒä¿®æ”¹] å…¨æ–°å‡ç´šçš„æ–‡å­—ç‰‡æ®µç”Ÿæˆå‡½å¼ï¼Œç‰¹åˆ¥å¼·åŒ–å°ä¸­æ–‡çš„æ”¯æ´ã€‚
    1.  **è¾¨è­˜ä¸­è‹±æ–‡æ¨™é»**ï¼šä½¿ç”¨åŒ…å«ã€Œã€‚ï¼›ï¼ï¼Ÿã€çš„å…¨å½¢æ¨™é»ä¾†é€²è¡Œæ›´è‡ªç„¶çš„æ–·å¥ã€‚
    2.  **æ»‘å‹•çª—å£åˆ‡åˆ†**ï¼šå¦‚æœä¸€å€‹é•·å¥å…§æ²’æœ‰æ¨™é»ï¼Œå‰‡æ¡ç”¨é‡ç–Šçš„ã€Œæ»‘å‹•çª—å£ã€æ–¹å¼åˆ‡åˆ†ï¼Œ
        ç¢ºä¿æ•´å¥è©±éƒ½æœƒè¢«å®Œæ•´è¦†è“‹åˆ°ï¼Œè€Œä¸æ˜¯åªå–é ­å°¾ã€‚
    3.  **å„ªåŒ–é•·åº¦åƒæ•¸**ï¼šèª¿æ•´äº†æœ€å°å’Œæœ€å¤§é•·åº¦ï¼Œæ›´é©åˆä¸­æ–‡çš„è³‡è¨Šå¯†åº¦ã€‚
    """
    t = _normalize_spaces(text)
    if not t:
        return []
    if len(t) <= max_len:
        return [t]

    # å„ªå…ˆå˜—è©¦ä½¿ç”¨ä¸­è‹±æ–‡æ¨™é»ç¬¦è™Ÿä¾†åˆ‡åˆ†å¥å­
    # (?<=[...]) æ˜¯æ­£è¦è¡¨ç¤ºå¼ä¸­çš„ "positive lookbehind"ï¼Œç¢ºä¿æ¨™é»ç¬¦è™Ÿæœ¬èº«ä¸è¢«åˆ‡æ‰
    sentences = re.split(r'(?<=[ã€‚ï¼Ÿï¼ï¼›!?;\.])\s*', t)
    valid_sentences = [s.strip() for s in sentences if s and s.strip() and len(s.strip()) >= min_len]

    # å¦‚æœæˆåŠŸæŒ‰æ¨™é»åˆ‡åˆ†å‡ºå¤šå€‹å¥å­ï¼Œå°±ä½¿ç”¨é€™å€‹çµæœ
    if len(valid_sentences) > 1 and any(len(s) < max_len for s in valid_sentences):
         return valid_sentences

    # å¦‚æœç„¡æ³•é æ¨™é»åˆ‡åˆ† (ä¾‹å¦‚ä¸€å€‹æ²’æœ‰æ¨™é»çš„è¶…é•·æ¢æ¬¾)
    # å‰‡ä½¿ç”¨é‡ç–Šçš„æ»‘å‹•çª—å£ (sliding window) æ–¹å¼ï¼Œç¢ºä¿è¦†è“‹å®Œæ•´å…§å®¹
    chunks = []
    # è¨­å®šé‡ç–Š20å€‹å­—å…ƒï¼Œè®“æ¨™è¨»æ›´é€£è²«
    overlap = 20 
    step = max_len - overlap

    for i in range(0, len(t), step):
        chunk = t[i:i + max_len]
        if chunk and len(chunk.strip()) > min_len:
            chunks.append(chunk.strip())

    return chunks if chunks else [t]

def _inset(rect, margin: float) -> "fitz.Rect":
    return fitz.Rect(rect.x0 + margin, rect.y0 + margin, rect.x1 - margin, rect.y1 - margin)

def build_highlighted_pdf(src_pdf_bytes: bytes, items: List[ClauseRisk], include_resolved: bool = False, resolved_idx: set = None) -> bytes:
    """PDF ç”¢ç”Ÿé‚è¼¯ï¼Œç¢ºä¿å°‡å®Œæ•´çš„ä¿®è¨‚æ¢æ–‡æ”¾å…¥è¨»è§£"""
    if resolved_idx is None: resolved_idx = set()
    doc = fitz.open(stream=src_pdf_bytes, filetype="pdf")
    not_found = []
    for idx, item in enumerate(items):
        if (not include_resolved and idx in resolved_idx) or item.risk not in ["HIGH", "MEDIUM"]:
            continue
        text_to_search = item.risk_sentence if item.risk_sentence else item.clause
        snippets = _candidate_snippets(text_to_search)

        # ã€é—œéµä¿®æ”¹ã€‘æ–°å¢ä¸€å€‹æ——æ¨™ï¼Œç”¨ä¾†è¿½è¹¤é€™å€‹é¢¨éšªé …ç›®æ˜¯å¦å·²ç¶“è¢«æ‰¾åˆ°ä¸¦æ¨™è¨»äº†
        item_found_and_annotated = False

        for page in doc:
            # å°æ–¼æ¯å€‹é é¢ï¼Œæˆ‘å€‘æª¢æŸ¥æ‰€æœ‰æ–‡å­—ç‰‡æ®µ
            for snip in snippets:
                if not isinstance(snip, str) or len(snip) < 5: continue
                rects = page.search_for(snip, quads=True)

                # å¦‚æœæ‰¾åˆ°äº†ä»»ä½•ä¸€å€‹ç‰‡æ®µ
                if rects:
                    # å°±åŸ·è¡Œæ¨™è¨»å’Œæ–°å¢è¨»è§£
                    annot = page.add_highlight_annot(rects)
                    annot.set_colors(stroke=(1, 0, 0) if item.risk == "HIGH" else (1, 0.55, 0))

                    info_content = f"ã€é¢¨éšªåŸå› ã€‘\n{item.reason}"
                    if item.suggestion and item.suggestion != "ç„¡éœ€ä¿®æ”¹":
                        info_content += f"\n\nã€å»ºè­°ä¿®è¨‚ç‰ˆæœ¬ã€‘\n{item.suggestion}"

                    annot.set_info(content=f"[{item.risk}] {info_content}")
                    annot.update()

                    # ã€é—œéµä¿®æ”¹ã€‘å°‡æ——æ¨™è¨­ç‚º Trueï¼Œè¡¨ç¤ºé€™å€‹é¢¨éšªé …ç›®å·²è™•ç†å®Œç•¢
                    item_found_and_annotated = True
                    # ã€é—œéµä¿®æ”¹ã€‘ç«‹åˆ»è·³å‡ºæœ€å…§å±¤çš„ã€Œç‰‡æ®µ(snippet)ã€è¿´åœˆ
                    break
            
            # ã€é—œéµä¿®æ”¹ã€‘å¦‚æœé€™å€‹é¢¨éšªé …ç›®å·²ç¶“è™•ç†å®Œç•¢ï¼Œä¹Ÿè·³å‡ºä¸­å±¤çš„ã€Œé é¢(page)ã€è¿´åœˆ
            if item_found_and_annotated:
                break

        # å¦‚æœéæ­·å®Œæ‰€æœ‰é é¢å¾Œï¼Œé€™å€‹é¢¨éšªé …ç›®ä»ç„¶æ²’æœ‰è¢«æ‰¾åˆ°
        if not item_found_and_annotated:
            not_found.append(item)

    if not_found:
        summary_page = doc.new_page()
        header = "Clauses Not Found For Highlighting"
        text_blocks = [header, ""]
        for miss in not_found:
            text_blocks.append(f"- {miss.risk} Â· {', '.join(miss.tags) or 'untagged'}")
            text_blocks.append(f"  Reason: {miss.reason}")
            text_blocks.append("  " + textwrap.shorten(_normalize_spaces(miss.clause), width=220))
            text_blocks.append("")
        inner_rect = _inset(summary_page.rect, 36)
        summary_page.insert_textbox(inner_rect, "\n".join(text_blocks), fontsize=10, align=0,fontname="china-tc")
    out = io.BytesIO()
    doc.save(out, deflate=True, garbage=4)
    doc.close()
    return out.getvalue()


# ---------------- App UI (ç„¡éœ€ä¿®æ”¹) ----------------
st.header("åˆç´„é¢¨éšªè©•é‘‘ Contract Risk Classifier")
st.markdown("ä¸Šå‚³åˆç´„ PDFï¼ŒAI å°‡è‡ªå‹•æ¨™ç¤º**é«˜/ä¸­**é¢¨éšªæ¢æ¬¾ï¼Œ**ç²¾æº–å®šä½é¢¨éšªå¥å­**ï¼Œä¸¦é™„ä¸Šåˆ†æèªªæ˜èˆ‡ä¿®è¨‚å»ºè­°ã€‚")
if "results" not in st.session_state: st.session_state.results = []
if "resolved" not in st.session_state: st.session_state.resolved = set()
if "source_pdf_bytes" not in st.session_state: st.session_state.source_pdf_bytes = None
left, right = st.columns([2, 1])
with left:
    uploaded = st.file_uploader("ä¸Šå‚³æ‚¨çš„åˆç´„ (.pdf)", type=["pdf"], key="contract_pdf")
    process_clicked = st.button("è™•ç†ä¸¦é€²è¡Œ AI é¢¨éšªåˆ†æ", type="primary", use_container_width=True, disabled=uploaded is None)
with right:
    cap = st.number_input("æœ€å¤§å¯åˆ†ææ¢æ¬¾æ•¸", min_value=5, max_value=50, value=20, step=5)
    split_method = st.selectbox("æ–‡å­—åˆ‡å‰²æ–¹å¼", options=["semantic", "regex", "recursive"], index=0)
    MODEL_NAME = st.selectbox("åˆ†ææ¨¡å‹ (é€²éš)", options=["gpt-4o", "gpt-4-turbo"], index=0)

if process_clicked:
    if not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY æœªè¨­å®šã€‚")
        st.stop()
    if uploaded is None:
        st.warning("è«‹å…ˆä¸Šå‚³ PDFã€‚")
        st.stop()

    try:
        pdf_bytes = uploaded.getvalue()
        st.session_state.source_pdf_bytes = pdf_bytes
        text = extract_text_from_pdf(io.BytesIO(pdf_bytes))
    except Exception as e:
        st.error(f"è®€å–æˆ–è§£æ PDF æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        st.stop()

    lang = get_language(text)
    st.caption(f"åµæ¸¬åˆ°æ–‡ä»¶ä¸»è¦èªè¨€ç‚º: **{'ä¸­æ–‡ (zh)' if lang == 'zh' else 'English (en)'}**")
    clauses = smart_split(text, method=split_method)[:cap]
    st.caption(f"ä½¿ç”¨ '{split_method}' æ–¹å¼åˆ‡å‰²å‡º {len(clauses)} å€‹æ¢æ¬¾å€å¡Šã€‚")
    if not clauses:
        st.warning("æœªèƒ½åœ¨æ–‡ä»¶ä¸­åˆ‡å‰²å‡ºæœ‰æ•ˆçš„æ¢æ¬¾ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æˆ–å˜—è©¦å…¶ä»–åˆ‡å‰²æ–¹å¼ã€‚")
        st.stop()

    all_results = classify_batch(clauses)

    high_medium_results = [r for r in all_results if r.risk in ["HIGH", "MEDIUM"]]
    noticeable_count = len(all_results) - len(high_medium_results)

    st.session_state.results = high_medium_results
    st.session_state.resolved = set()

    st.session_state.last_run_message = f"åˆ†æå®Œæˆï¼å·²è­˜åˆ¥å‡º {len(high_medium_results)} å€‹é«˜/ä¸­é¢¨éšªé …ç›®ã€‚"
    if noticeable_count > 0:
        st.session_state.last_run_message += f" (å·²è‡ªå‹•éæ¿¾ {noticeable_count} å€‹ä½é¢¨éšªé …ç›®)"

    st.rerun()

if "last_run_message" in st.session_state:
    st.success(st.session_state.last_run_message)
    del st.session_state.last_run_message

if st.session_state.results:
    results: List[ClauseRisk] = st.session_state.results
    resolved: set = st.session_state.resolved
    def mark_resolved(idx: int): st.session_state.resolved.add(idx)
    def undo_resolved(idx: int): st.session_state.resolved.discard(idx)
    active_indices = [i for i in range(len(results)) if i not in resolved]
    active_results = [results[i] for i in active_indices]

    counts = {"HIGH": sum(1 for r in active_results if r.risk == "HIGH"),
              "MEDIUM": sum(1 for r in active_results if r.risk == "MEDIUM")}

    st.divider()
    st.subheader("é¢¨éšªæ‘˜è¦ Summary")
    st.write(f"ğŸ”´ **é«˜é¢¨éšª: {counts['HIGH']}** Â· ğŸŸ  **ä¸­é¢¨éšª: {counts['MEDIUM']}** Â· âœ… **å·²è§£æ±º: {len(resolved)}**")

    show_resolved = st.checkbox("é¡¯ç¤ºå·²è§£æ±ºé …ç›®", value=False)

    badge_map = {"HIGH": "ğŸ”´ é«˜é¢¨éšª HIGH RISK", "MEDIUM": "ğŸŸ  ä¸­é¢¨éšª MEDIUM RISK"}

    def render_card(idx: int, item: ClauseRisk, is_resolved: bool):
        with st.container(border=True):
            left_col, right_col = st.columns([8, 2])
            with left_col:
                status = "âœ… å·²è§£æ±º RESOLVED" if is_resolved else badge_map.get(item.risk, "ğŸŸ  ä¸­é¢¨éšª MEDIUM RISK")
                st.markdown(f"**{status}** Â· *Tags: {', '.join(item.tags) or 'ç„¡'}*")
                if item.risk_sentence and not is_resolved:
                    st.markdown("##### ğŸ”‘ é¢¨éšªæ ¹æºå¥ (Risk Root-Cause Sentence)")
                    st.markdown(f"> {item.risk_sentence}")

                st.markdown("##### ğŸ’¬ AI åˆ†æèˆ‡ç†ç”± (AI Analysis & Reason)")
                st.info(f"{item.reason}")

                if item.suggestion and item.suggestion != "ç„¡éœ€ä¿®æ”¹" and not is_resolved:
                    st.markdown("##### âœï¸ å»ºè­°ä¿®è¨‚ç‰ˆæœ¬ (Suggested Full Revision)")
                    st.success(f"{item.suggestion}")

                with st.expander("æª¢è¦–å®Œæ•´æ¢æ¬¾ä¸Šä¸‹æ–‡ (View Full Clause Context)"):
                    st.text_area("Clause Text", value=item.clause, height=150, disabled=True, key=f"clause_text_{idx}")

            with right_col:
                if not is_resolved:
                    st.button("Resolved", key=f"resolve_btn_{idx}", help="å°‡æ­¤é …ç›®æ¨™ç¤ºç‚ºå·²è§£æ±º", on_click=mark_resolved, args=(idx,), use_container_width=True)
                else:
                    st.button("Undo", key=f"undo_btn_{idx}", help="å°‡æ­¤é …ç›®ç§»å›å¾…è™•ç†æ¸…å–®", on_click=undo_resolved, args=(idx,), use_container_width=True)

    for i in active_indices:
        render_card(i, results[i], is_resolved=False)
    if show_resolved and resolved:
        st.markdown("### Resolved Items")
        for i in sorted(list(resolved)):
            render_card(i, results[i], is_resolved=True)

    st.divider()
    st.subheader("åŒ¯å‡ºé‡é»æ¨™è¨˜ Export with Highlights (PDF)")
    if st.session_state.source_pdf_bytes:
        include_resolved_pdf = st.toggle("åœ¨ PDF ä¸­åŒ…å«å·²è§£æ±ºé …ç›®", value=False)
        try:
            pdf_bytes = build_highlighted_pdf(
                st.session_state.source_pdf_bytes,
                items=results,
                include_resolved=include_resolved_pdf,
                resolved_idx=resolved
            )
            st.download_button(
                "åŒ¯å‡ºå«é‡é»æ¨™è¨»çš„ PDF",
                data=pdf_bytes, file_name="contract_highlighted.pdf",
                mime="application/pdf", use_container_width=True,
            )
        except Exception as e:
            st.error(f"ç”¢ç”Ÿ PDF æ¨™ç¤ºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
