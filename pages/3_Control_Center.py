# pages/Control_Center.py
import os, tempfile, json
from datetime import datetime
import streamlit as st
import pandas as pd
from streamlit_option_menu import option_menu
from dotenv import load_dotenv

# LangChain / Vector DB
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

# ---------- Page setup ----------
st.set_page_config(page_title="Control Center", layout="wide")
load_dotenv()
INDEX_NAME = "contract-assistant"

# ---------- Session defaults ----------
if "search_history" not in st.session_state:
    st.session_state.search_history = [
        {"id":"q-001","query":"æ˜Ÿå®‡èˆªç©ºåˆç´„åˆ†æ","timestamp":"2025-06-28T10:34:00",
         "results":[{"title":"Q2 æ‘˜è¦","snippet":"ç‡Ÿæ”¶å¹´å¢ 12%","path":"reports/q2_2024.pdf"}],
         "tags":["finance","q2"],"pinned":True,"notes":"è‘£äº‹æœƒç°¡å ±ç”¨"},
        {"id":"q-002","query":"å®¢æˆ¶æµå¤±å„€è¡¨æ¿","timestamp":"2025-07-03T14:09:20",
         "results":[{"title":"Cohort åˆ†æ","snippet":"Janâ€“Jun","path":"dashboards/churn.html"}],
         "tags":["product","retention"],"pinned":False,"notes":""},
    ]
if "processed_namespaces" not in st.session_state:
    st.session_state.processed_namespaces = []
if "selected_namespace" not in st.session_state:
    st.session_state.selected_namespace = None
if "core_points_text" not in st.session_state:
    st.session_state.core_points_text = ""
if "comparison_results" not in st.session_state:
    st.session_state.comparison_results = None
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7
if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 256

# ---------- Helpers ----------
def _find_by_id(qid):
    for it in st.session_state.search_history:
        if it["id"] == qid: return it
    return None

def _df(items):
    return pd.DataFrame([{
        "id": it["id"],
        "query": it["query"],
        "timestamp": it["timestamp"],
        "tags": ", ".join(it.get("tags", [])),
        "pinned": it.get("pinned", False),
        "top_title": (it["results"][0]["title"] if it.get("results") else ""),
        "top_path": (it["results"][0]["path"] if it.get("results") else ""),
    } for it in items])

@st.cache_resource
def get_pinecone_client():
    return Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

def fetch_pinecone_namespaces(index_name):
    pc = get_pinecone_client()
    try:
        stats = pc.describe_index(index_name).stats
        return list(stats.namespaces.keys()) if stats and stats.namespaces else []
    except Exception:
        return []

if not st.session_state.processed_namespaces:
    st.session_state.processed_namespaces = fetch_pinecone_namespaces(INDEX_NAME)

def process_and_ingest_reference_file(uploaded_file):
    namespace = uploaded_file.name
    with st.spinner(f"æ­£åœ¨è™•ç†åƒè€ƒæ–‡ä»¶ '{namespace}' ä¸¦å­˜å…¥æ°¸ä¹…çŸ¥è­˜åº«..."):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            path = tmp.name
        loader = PyPDFLoader(path)
        documents = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
        docs = splitter.split_documents(documents)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        PineconeVectorStore.from_documents(docs, embedding=embeddings,
                                           index_name=INDEX_NAME, namespace=namespace)
        os.remove(path)
    st.success(f"åƒè€ƒæ–‡ä»¶ '{namespace}' å·²æˆåŠŸå­˜å…¥çŸ¥è­˜åº«ï¼")
    if namespace not in st.session_state.processed_namespaces:
        st.session_state.processed_namespaces.append(namespace)
    st.cache_data.clear()

@st.cache_resource
def load_and_process_pdf_for_faiss(_uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(_uploaded_file.getvalue())
        path = tmp.name
    loader = PyPDFLoader(path)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(split_docs, embeddings)
    os.remove(path)
    return vs.as_retriever(search_kwargs={'k': 2})

def run_comparison(template_retriever, uploaded_retriever, review_points, temperature, max_tokens):
    llm = ChatOpenAI(model_name='gpt-4o', temperature=temperature, max_tokens=max_tokens)
    tpl = """
ä½ æ˜¯ä¸€ä½é ‚å°–çš„æ³•å‹™å°ˆå®¶ï¼Œä½ çš„ä»»å‹™æ˜¯ç²¾æº–æ¯”è¼ƒå…©ä»½åˆç´„ä¸­çš„åŒä¸€æ¢æ¬¾ï¼Œä¸¦ä»¥ä¿è­·ã€Œæˆ‘æ–¹å…¬å¸ã€çš„åˆ©ç›Šç‚ºæœ€é«˜åŸå‰‡ã€‚
**æˆ‘æ–¹å…¬å¸çš„æ¨™æº–ç¯„æœ¬æ¢æ¬¾:**
```{template_clause}```
**å¾…å¯©æ–‡ä»¶çš„å°æ‡‰æ¢æ¬¾:**
```{uploaded_clause}```
è«‹é‡å°ã€Œ{topic}ã€é€™å€‹å¯©æŸ¥é‡é»ï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
1. **æ¢æ¬¾æ‘˜è¦**, 2. **å·®ç•°åˆ†æ**, 3. **é¢¨éšªæç¤ºèˆ‡å»ºè­°**ã€‚
è«‹ç”¨ Markdown æ ¼å¼æ¸…æ™°åœ°å‘ˆç¾ä½ çš„åˆ†æå ±å‘Šã€‚
"""
    prompt = PromptTemplate.from_template(tpl)
    chain = prompt | llm | StrOutputParser()
    results = {}
    progress = st.progress(0, text="é–‹å§‹é€²è¡Œæ¯”å°...")
    for i, topic in enumerate(review_points):
        t_docs = template_retriever.invoke(topic)
        u_docs = uploaded_retriever.invoke(topic)
        t_text = "\n---\n".join([d.page_content for d in t_docs])
        u_text = "\n---\n".join([d.page_content for d in u_docs])
        results[topic] = chain.invoke({"topic": topic,
                                       "template_clause": t_text,
                                       "uploaded_clause": u_text})
        progress.progress((i + 1) / len(review_points), text=f"æ­£åœ¨åˆ†æ: {topic}")
    progress.empty()
    return results

# ---------- UI ----------
st.title("æ§åˆ¶ä¸­å¿ƒ Control Center")

pins = sum(1 for it in st.session_state.search_history if it.get("pinned"))

top = option_menu(
    None, ["Settings", "Search"],
    icons=["gear-fill", "search"],
    menu_icon="cast", default_index=1, orientation="horizontal",
)

# ----- Settings -----
if top == "Settings":
    st.subheader('æ¨¡å‹åƒæ•¸ Model Parameters')
    st.session_state.temperature = st.slider("åƒæ•¸æº«åº¦ Temperature", 0.0, 2.0, st.session_state.temperature, 0.1)
    with st.expander("What does temperature do?"):
        st.caption("Lower = focused & predictable; higher = varied & creative.")
    st.session_state.max_tokens = st.slider("æœ€å¤§å­—å…ƒæ•¸ Max Tokens", 1, 4096, st.session_state.max_tokens)
    with st.expander("What do tokens do?"):
        st.caption("Max Tokens limits the length of AI responses.")
    st.toggle("Streaming responses", value=True)
    st.text_input("System prompt preset", placeholder="You are a helpful analystâ€¦")

# ----- Search -----
if top == "Search":
    st.subheader("æœå°‹æ§åˆ¶å° Search Console")
    sub = option_menu(
        None, ["Query History", f"Pinned ({pins})", "Tools"],
        icons=["clock-history", "pin-angle-fill", "wrench-adjustable-circle"],
        menu_icon="cast", default_index=0, orientation="horizontal",
    )

    if sub == "Query History":
        q = st.text_input("Search your past queriesâ€¦", placeholder="Type to filter in real-time")
        c1, c2 = st.columns([3, 1])
        tag_filter = c1.text_input("Filter by tag", placeholder="finance / product / â€¦")
        only_pinned = c2.toggle("Pinned only", value=False)
        items = st.session_state.search_history
        if q: items = [it for it in items if q.lower() in it["query"].lower()]
        if tag_filter: items = [it for it in items if tag_filter.lower() in [t.lower() for t in it.get("tags", [])]]
        if only_pinned: items = [it for it in items if it.get("pinned")]
        items = sorted(items, key=lambda x: x["timestamp"], reverse=True)
        if not items:
            st.info("No matching history.")
        else:
            for it in items:
                with st.expander(f"**{it['query']}** Â· {datetime.fromisoformat(it['timestamp']).strftime('%b %d, %Y %H:%M')}"):
                    st.toggle(
                        "Pinned", key=f"pin_{it['id']}", value=it.get("pinned"),
                        on_change=lambda _id=it["id"]: _find_by_id(_id).__setitem__("pinned", not _find_by_id(_id).get("pinned"))
                    )

    if sub.startswith("Pinned"):
        df = _df([it for it in st.session_state.search_history if it.get("pinned")])
        st.dataframe(df, use_container_width=True, hide_index=True) if not df.empty else st.info("No pinned items yet.")

    if sub == "Tools":
        all_df = _df(st.session_state.search_history)
        st.markdown("##### Quick Exports")
        c1, c2 = st.columns(2)
        c1.download_button("Export all (JSON)",
                           json.dumps(st.session_state.search_history, ensure_ascii=False, indent=2).encode("utf-8"),
                           file_name="search_history.json")
        c2.download_button("Export all (CSV)",
                           all_df.to_csv(index=False).encode("utf-8"),
                           file_name="search_history.csv", mime="text/csv")

# ----- Results -----
if st.session_state.get("comparison_results"):
    st.subheader("ğŸ“œ åˆç´„æ¯”å°åˆ†æå ±å‘Š")
    for topic, result in st.session_state.comparison_results.items():
        with st.expander(f"**å¯©æŸ¥é …ç›®ï¼š{topic}**", expanded=True):
            st.markdown(result, unsafe_allow_html=True)
    st.session_state.comparison_results = None
